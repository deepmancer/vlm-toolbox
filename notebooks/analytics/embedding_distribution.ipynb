{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7795548e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb376c-7780-4444-92dd-bec7abbafeac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../vlm_toolbox/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384a42e-86be-4702-a728-36f97df23c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6bcea-fa71-4772-8a72-37c94b9e58d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import warnings\n",
    "\n",
    "import holoviews as hv\n",
    "import pandas as pd\n",
    "import torch\n",
    "from cuml import TSNE\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "from config.annotations import AnnotationsConfig\n",
    "from config.enums import (\n",
    "    CLIPBackbones,\n",
    "    DataStatus,\n",
    "    ImageDatasets,\n",
    "    Modalities,\n",
    "    ModalityType,\n",
    "    ModelType,\n",
    "    Setups,\n",
    "    Stages,\n",
    "    Trainers,\n",
    ")\n",
    "from config.image_datasets import ImageDatasetConfig\n",
    "from config.model import ModelConfigManager\n",
    "from config.path import VISUALIZATIONS_ROOT_DIR\n",
    "from config.setup import Setup\n",
    "from data.data_access.image_factory import ImageHandlerFactory\n",
    "from data.data_access.label_factory import LabelHandleFactory\n",
    "from data.data_access.text_factory import TextHandlerFactory\n",
    "from model.vlm_factory import VLMFactory\n",
    "from util.color import generate_diverse_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269eea6-a0ab-4310-959a-d285de5d8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "warnings.filterwarnings('ignore')\n",
    "hv.extension('bokeh', 'matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26a5d8-5d11-4f44-a2e7-0559c0b9e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff06f44-2de8-4cb8-bfa8-084bf9343699",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d573ee90-735b-45df-b358-4795a67b7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = '/home/alireza/novel_coop/few_shot/imagenet1k/clip_vit_b_16/open_ai/coop/16_shots/default/novel/pytorch_model.bin'\n",
    "OUTPUT_DIR = VISUALIZATIONS_ROOT_DIR + 'embeds_tsne/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54070c-e6f9-4173-8f59-1745d6b69ef7",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f939e-7518-4a25-8b94-be7f0e3a812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_TYPE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE = torch.device(DEVICE_TYPE)\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a7eba-454f-4275-bb65-817344781306",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720414f6-631a-499d-838a-487ccb7afc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESS_BATCH_SIZE = 512\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd60d7-92e0-43c4-8d35-19a1ae52aa83",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1e4f91-d431-48a7-a52a-b4f923d3b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = Setup(\n",
    "    dataset_name=ImageDatasets.IMAGENET_1K,\n",
    "    backbone_name=CLIPBackbones.CLIP_VIT_B_16,\n",
    "    trainer_name=Trainers.COOP,\n",
    "    model_type=ModelType.FEW_SHOT,\n",
    "    setup_type=Setups.FULL,\n",
    "    num_epochs=200,\n",
    "    train_batch_size=1000,\n",
    "    eval_batch_size=1024,\n",
    "    validation_size=0.15,\n",
    "    n_shots=16,\n",
    "    coarse_column_name='coarse',\n",
    "    # annotations_key_value_criteria={'kingdom': ['Animalia']},\n",
    "    # top_k=67,\n",
    "    # model_checkpoint_path='/home/alireza/io/model/few_shot/imagenet1k/clip_vit_b_16/open_ai/coop/16_shots/coarse/novel/pytorch_model.binpytorch_model.bin'\n",
    "    # model_checkpoint_path='/home/alireza/io/model/zero_shot/imagenet1k/clip_vit_b_16/open_ai/coop/16_shots/default/pytorch_model.bin'\n",
    ")\n",
    "setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b86269-b667-496f-9eac-14f5d7a0d6be",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e980ee0-f286-42a3-be75-4a0fea0dd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_MODALITY_TYPE = DataStatus.EMBEDDING\n",
    "SPLIT = Stages.EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443fd19-436c-41d5-b371-3067b487f875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotations_config = AnnotationsConfig.get_config(dataset_name=setup.dataset_name)\n",
    "image_dataset_config = ImageDatasetConfig.get_config(\n",
    "    setup,\n",
    "    split=SPLIT,\n",
    "    data_type=IMAGE_MODALITY_TYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada00bb1-fa99-4004-9e51-da9e7180d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOFT_PROMPT_GROUP = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e208ec1-b43f-4cb3-9ce5-0622d7b33467",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec2722-dd34-4993-b1dc-b21f0281e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_embeddings_tsne_visualization(visualization_df, setup, modality_types=ModalityType.get_values(), hue_col_name='coarse', directory=OUTPUT_DIR, show_plot=False):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    categories = visualization_df[hue_col_name].unique()\n",
    "    colors = generate_diverse_colors(len(categories))\n",
    "    color_map = {category: c for category, c in zip(categories, colors)}\n",
    "\n",
    "    traces = []\n",
    "    trace_types = []\n",
    "    for category in categories:\n",
    "        df_filtered = visualization_df[visualization_df[hue_col_name] == category]\n",
    "        for modality in modality_types:\n",
    "            df_modality = df_filtered[df_filtered['type'] == modality]\n",
    "            hover_text = df_modality.apply(lambda row: f\"label: {row['label']}<br>label_id: {row['label_id']}\", axis=1)\n",
    "            marker_size = 5 if modality == ModalityType.TEXT else 2\n",
    "            opacity = 0.8 if modality == ModalityType.TEXT else 0.6\n",
    "\n",
    "            trace = go.Scatter(\n",
    "                x=df_modality['x'],\n",
    "                y=df_modality['y'],\n",
    "                mode='markers',\n",
    "                marker=dict(size=marker_size, opacity=opacity, color=color_map[category]),\n",
    "                name=f\"{category} - {modality}\",\n",
    "                legendgroup=category,\n",
    "                text=hover_text,\n",
    "                hoverinfo='text+name'\n",
    "            )\n",
    "            traces.append(trace)\n",
    "            trace_types.append(modality)\n",
    "\n",
    "    updatemenus = [\n",
    "        {\n",
    "            \"type\": \"buttons\",\n",
    "            \"buttons\": [\n",
    "                {\"label\": \"All\", \"method\": \"update\", \"args\": [{\"opacity\": [0.6 if modality == ModalityType.IMAGE else 0.8 for modality in trace_types]}]},\n",
    "                {\"label\": \"Image Only\", \"method\": \"update\", \"args\": [{\"opacity\": [0.6 if modality == ModalityType.IMAGE else 0 for modality in trace_types]}]},\n",
    "                {\"label\": \"Text Only\", \"method\": \"update\", \"args\": [{\"opacity\": [0.8 if modality == ModalityType.TEXT else 0 for modality in trace_types]}]}\n",
    "            ],\n",
    "           \n",
    "            \"direction\": \"down\",\n",
    "            \"showactive\": True,\n",
    "        },\n",
    "        {\n",
    "            \"type\":\"buttons\",\n",
    "            \"buttons\": [\n",
    "                {\"label\": \"Select All\", \"method\": \"update\", \"args\": [{\"visible\": True}]},\n",
    "                {\"label\": \"Hide All\", \"method\": \"update\", \"args\": [{\"visible\": \"legendonly\"}]}\n",
    "            ],\n",
    "            \"direction\": \"left\",\n",
    "            \"showactive\": True,\n",
    "            \"x\": 0.5,\n",
    "            \"xanchor\": 'center',\n",
    "            \"y\": 1.1,\n",
    "            \"yanchor\": 'top'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title='Combined t-SNE Visualization',\n",
    "        showlegend=True,\n",
    "        xaxis=dict(title='t-SNE Dimension 1'),\n",
    "        yaxis=dict(title='t-SNE Dimension 2'),\n",
    "        updatemenus=updatemenus\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "    fig.update_layout(\n",
    "        legend_title_text=hue_col_name,\n",
    "        legend={'itemsizing': 'constant', 'groupclick': \"toggleitem\"}\n",
    "    )\n",
    "\n",
    "    file_name = f'{setup.backbone_name}_{setup.trainer_name}_image_text_embeds_tsne.html'\n",
    "    full_path = os.path.join(directory, file_name)\n",
    "    file_index = 1\n",
    "    \n",
    "    while os.path.exists(full_path):\n",
    "        base_name, extension = os.path.splitext(file_name)\n",
    "        new_file_name = f\"{base_name}_{file_index}{extension}\"\n",
    "        full_path = os.path.join(directory, new_file_name)\n",
    "        file_index += 1\n",
    "\n",
    "    fig.write_html(full_path)\n",
    "    print(f'saved in {full_path}')\n",
    "    if show_plot:\n",
    "        fig.update_layout(\n",
    "            width=800,\n",
    "            height=600,\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "def create_visualization_df(label_handler, image_embeds_tsne, image_embeds_id, text_embeds_tsne, text_embeds_id, coarse_col_name='coarse'):\n",
    "    mapping = label_handler.get_mapping('class_id', 'label_id').int().numpy()\n",
    "    df_images = pd.DataFrame(image_embeds_tsne, columns=['x', 'y'])\n",
    "    if len(image_embeds_id):\n",
    "        df_images['label_id'] = image_embeds_id\n",
    "        df_images['label_id'] = df_images['label_id'].apply(lambda class_id: mapping[class_id])\n",
    "    df_images['type'] = ModalityType.IMAGE\n",
    "    df_texts = pd.DataFrame(text_embeds_tsne, columns=['x', 'y'])\n",
    "    if len(text_embeds_id):\n",
    "        df_texts['label_id'] = text_embeds_id\n",
    "    df_texts['type'] = ModalityType.TEXT\n",
    "    df_combined = pd.concat([df_images, df_texts], ignore_index=True).reset_index(drop=True)\n",
    "    df_combined = (\n",
    "        df_combined\n",
    "        .merge(\n",
    "            label_handler.labels_df[['label_id', coarse_col_name, 'label']].drop_duplicates(),\n",
    "            on='label_id',\n",
    "            how='left',\n",
    "        )\n",
    "    )\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d480971a-2b41-4cc9-afb8-4a1338dfdfb7",
   "metadata": {},
   "source": [
    "# Labels Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe52c43-e230-422e-9f0a-c475c2c60388",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_handler = (\n",
    "    LabelHandleFactory.create_from_config(annotations_config)\n",
    "    .set_prompt_mode(is_soft=setup.get_is_soft())\n",
    "    .config_prompts()\n",
    ").show()\n",
    "\n",
    "labels = label_handler.get_labels()\n",
    "labels_df = label_handler.get_labels_df()\n",
    "prompts_df = label_handler.get_prompts_df()\n",
    "class_ids = label_handler.get_class_ids()\n",
    "class_id_label_id_adj_matrix = label_handler.get_class_id_label_id_adj_matrix()\n",
    "label_id_prompt_id_mapping = label_handler.get_label_id_prompt_id_mapping()\n",
    "\n",
    "classes_df = label_handler.get_classes_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c936b6-bc1e-4e25-b041-18322c8d28bb",
   "metadata": {},
   "source": [
    "# Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06791fe9-01b6-4e81-a957-ce78707ae37c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_config = ModelConfigManager.get_config(\n",
    "    backbone_name=setup.backbone_name,\n",
    "    source=setup.source,\n",
    "    context_initialization=None,\n",
    "    trainer_name=setup.trainer_name,\n",
    "    labels=labels,\n",
    "    label_id_prompt_id_mapping=label_id_prompt_id_mapping,\n",
    ")\n",
    "\n",
    "vlm = VLMFactory.from_pretrained(model_config=model_config).to(DEVICE).eval()\n",
    "# if CHECKPOINT_PATH:\n",
    "#     vlm.load_state_dict(torch.load(CHECKPOINT_PATH))\n",
    "vlm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa06f54-5988-40da-b5b4-e3bc56426b21",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dda791-0cac-4e48-a100-b74ebe7f388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset_handler = (\n",
    "    ImageHandlerFactory.create_from_config(\n",
    "        key=Modalities.M1,\n",
    "        stage='validation',\n",
    "        dataset_config=image_dataset_config,\n",
    "        to_keep_ids=class_ids,\n",
    "    )\n",
    "    # .to_few_shot_dataset(16)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb560dc-d617-42f9-b885-6c11b8c7c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset_handler = TextHandlerFactory.create_from_df(\n",
    "    Modalities.M2,\n",
    "    SPLIT,\n",
    "    prompts_df,\n",
    "    annotations_config,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad4be7-a981-4597-bcb2-0f482bdc3ac6",
   "metadata": {},
   "source": [
    "### Pre-compute Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b7ea28-ce53-4997-94fe-20345ea1ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_handler in [image_dataset_handler, text_dataset_handler]:\n",
    "    if not dataset_handler.is_embedded():\n",
    "        with torch.no_grad(), torch.autocast(device_type=DEVICE_TYPE, dtype=torch.float16):\n",
    "            vlm.eval()\n",
    "            dataset_handler.to_embedding(\n",
    "                vlm.get_embedding_fn_for_modality(dataset_handler.modality),\n",
    "                batch_size=PREPROCESS_BATCH_SIZE,\n",
    "            )\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b83edb6-bcba-4c80-b62b-a6708085e335",
   "metadata": {},
   "source": [
    "# T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7127f-bc56-4174-88f3-ee348a1a051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset, image_dataset = text_dataset_handler.get_dataset(), image_dataset_handler.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040f142-573b-426e-a630-5a021b4f1785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_embeds = image_dataset['image_embeds']\n",
    "text_embeds = text_dataset['text_embeds']\n",
    "\n",
    "image_embeds /= image_embeds.norm(dim=-1, keepdim=True)\n",
    "text_embeds /= text_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "image_embeds_id = image_dataset['class_id'].int().numpy()\n",
    "text_embeds_id = text_dataset['label_id'].int().numpy()\n",
    "all_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa75514-b375-48db-a37a-7c8e78ffedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dimension_reduction(image_embeds, text_embeds, image_ids, text_ids, separate=False):\n",
    "    if separate:\n",
    "        text_embeds_tsne = TSNE(n_components=2, metric='cosine', init='pca').fit_transform(text_embeds.numpy())\n",
    "        image_embeds_tsne = TSNE(n_components=2, metric='cosine', init='pca').fit_transform(image_embeds.numpy())\n",
    "    else:\n",
    "        all_embeds = torch.concatenate((text_embeds, image_embeds), dim=0).numpy()\n",
    "        all_embeds_tsne = TSNE(n_components=2, metric='cosine', init='pca', n_neighbors=num_neighbors).fit_transform(all_embeds)\n",
    "        image_embeds_tsne = all_embeds_tsne[:image_ids.shape[0]]\n",
    "        text_embeds_tsne = all_embeds_tsne[image_ids.shape[0]:]\n",
    "    return text_embeds_tsne, image_embeds_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac274149-1201-4a01-8b69-a3e38b1055f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n text_embeds_tsne, image_embeds_tsne = um_neighbors = len(image_dataset) // len(image_dataset.unique('class_id')) + 1\n",
    "text_embeds_tsne, image_embeds_tsne = perform_dimension_reduction(image_embeds, text_embeds, image_embeds_id, text_embeds_id, separate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc7970-f161-41b2-ad2c-f662eceabf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_df = create_visualization_df(\n",
    "    label_handler,\n",
    "    image_embeds_tsne,\n",
    "    image_embeds_id,\n",
    "    text_embeds_tsne,\n",
    "    text_embeds_id,\n",
    "    coarse_col_name='coarse',\n",
    ")\n",
    "visualization_df.sample(frac=0.1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed7d5a-8164-4054-9a78-add1c44596eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_embeddings_tsne_visualization(visualization_df, modality_types=[ModalityType.TEXT], hue_col_name='coarse', setup=setup, show_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60e82a9-babe-4356-ad7d-916421b66e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3e7de-9e2e-440c-83f2-55643dbac9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clip] *",
   "language": "python",
   "name": "conda-env-clip-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
