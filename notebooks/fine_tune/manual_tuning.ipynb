{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc7e716-f7f4-42a8-bdec-0a40a4b2d405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alireza/vlm_toolbox\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../../vlm_toolbox/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ec644e8-79c4-41df-9187-7f11e765181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795548e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3406a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from config.annotations import AnnotationsConfig\n",
    "from config.enums import (\n",
    "    CLIPBackbones,\n",
    "    ImageDatasets,\n",
    "    Modalities,\n",
    "    ModelType,\n",
    "    PrecisionDtypes,\n",
    "    Setups,\n",
    "    Sources,\n",
    "    Stages,\n",
    "    Trainers,\n",
    ")\n",
    "from config.image_datasets import ImageDatasetConfig\n",
    "from config.setup import Setup\n",
    "from config.train import TrainingArgumentsConfig\n",
    "from data.data_access.image_factory import ImageHandlerFactory\n",
    "from data.data_access.label_factory import LabelHandleFactory\n",
    "from metric.accuracy import AccuracyMetricEvaluator\n",
    "from util.path import mkdir_if_missing\n",
    "from util.torch_helper import describe_model, set_module_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1399e-8c0a-49c1-9e06-a71e4de657f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "959be6b9-c2be-4f5b-98d7-0722f62c84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c82c9",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97a899-19c7-4e00-ad98-190f77e33e23",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec2bc090-ba2e-4553-ad44-3aa53f38914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dset_dir = '/home/alireza/io/imagenet1k/embedding/dyno_v2_giant/huggingface/validation/'\n",
    "mkdir_if_missing(embed_dset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd5ffc-7d92-49ba-8e44-c60e806f0e06",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53de8715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Setup: (setup_type='full', dataset_name='imagenet1k', backbone_name='clip_vit_b_16', source='open_ai', trainer_name='clip', model_type='zero_shot', annotations_key_value_criteria={}, precision_dtype='fp16', metric_for_best_model='top_1', n_shots=20, enable_novelty=False, train_batch_size=512, eval_batch_size=1024, preprocess_batch_size=1024, train_split='train', m1_m2_id_same_granularity_level=False, eval_split='validation', top_k=5, random_state=42, is_soft=False, train_full_precision=False, eval_full_precision=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup = Setup(\n",
    "    setup_type=Setups.FULL,\n",
    "    model_type=ModelType.ZERO_SHOT,\n",
    "    dataset_name=ImageDatasets.IMAGENET_1K,\n",
    "    backbone_name=CLIPBackbones.CLIP_VIT_B_16,\n",
    "    trainer_name=Trainers.CLIP,\n",
    "    source=Sources.OPEN_AI,\n",
    "    metric_for_best_model=AccuracyMetricEvaluator.get_main_metric_name(),\n",
    "    eval_split=Stages.EVAL,\n",
    "    train_split=Stages.TRAIN,\n",
    "    eval_batch_size=1024,\n",
    "    n_shots=20,\n",
    "    # label_column_name='family',\n",
    "    precision_dtype=PrecisionDtypes.FP16,\n",
    ")\n",
    "setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531bfb4-3c5d-4ba5-97fc-04f060fa0c30",
   "metadata": {},
   "source": [
    "# Labels Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94fbb5a0-a115-44cc-b0e1-21fc5ce8cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_config = AnnotationsConfig.get_config(dataset_name=setup.get_dataset_name())\n",
    "label_handler = LabelHandleFactory.create_from_config(annotations_config)\n",
    "label_handler = (\n",
    "    label_handler\n",
    "    .config_prompts()\n",
    ")\n",
    "class_id_label_id_adj_matrix = label_handler.get_class_id_label_id_adj_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10492c19-cf6b-47d2-90d1-1a716bdc1bf9",
   "metadata": {},
   "source": [
    "### Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a085a73e-2ba8-41cb-9a15-9fe603107f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dataset_config = ImageDatasetConfig.get_config(\n",
    "    setup,\n",
    "    split='validation',\n",
    "    data_type='raw',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bf1d8de-5be1-43a5-b35d-9ec36cd48d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'class_id'],\n",
      "    num_rows: 50000\n",
      "})\n",
      "\n",
      "Identifier column name: class_id\n",
      "Value column names: ['image']\n",
      "\n",
      "Modality(modality_type='image', identifier='class_id', raw_keys=['image'], preprocessed_keys={'input': ['image'], 'output': ['pixel_values']}, key='m1', stage='validation', embedding_key={'input': ['pixel_values'], 'output': ['image_embeds']}, status='raw', requires_grad=true, requires_preprocess=true, perform_augmentation=false)\n"
     ]
    }
   ],
   "source": [
    "train_image_dataset_handler = ImageHandlerFactory.create_from_config(\n",
    "    Modalities.M1,\n",
    "    'validation',\n",
    "    train_image_dataset_config,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa65767-9031-4a5a-ac61-80d66cd74f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters: 1136480768\n",
      "ðŸ”¥ Trainable model parameters: 0 (0.00%)\n",
      "ðŸ§Š Frozen model parameters: 1136480768 (100.00%)\n",
      "\n",
      "Attribute Name: embeddings, Type: Dinov2Embeddings, Param Share: 0.27%:\n",
      "-    Attribute Name: patch_embeddings, Type: Dinov2PatchEmbeddings, Param Share: 30.04%:\n",
      "--       ðŸ”¥ Trainable: 0.00%, ðŸ§Š Frozen: 100.00%\n",
      "-    Attribute Name: dropout, Type: Dropout, Param Share: 0.00%:\n",
      "--       ðŸ”¥ Trainable: 0.00%, ðŸ§Š Frozen: 0.00%\n",
      "\n",
      "Attribute Name: encoder, Type: Dinov2Encoder, Param Share: 99.73%:\n",
      "-    Attribute Name: layer, Type: ModuleList, Param Share: 100.00%:\n",
      "--       ðŸ”¥ Trainable: 0.00%, ðŸ§Š Frozen: 100.00%\n",
      "\n",
      "Attribute Name: layernorm, Type: LayerNorm, Param Share: 0.00%:\n",
      "\n",
      "Device: cuda:0\n",
      "Model size: 4335.330MB\n"
     ]
    }
   ],
   "source": [
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        outputs = self.model(*args, **kwargs)\n",
    "        return outputs[1]\n",
    "    \n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-giant')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-giant').eval().to(torch.device('cuda'))\n",
    "set_module_trainable(model, set_trainable=False)\n",
    "describe_model(model)\n",
    "wrapped_model = ModelWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "201fe8a7-83b9-4d5f-9aed-3daffa2d9f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938443cf7157479ca0d531799d9fcf06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = train_image_dataset_handler.get_dataset()\n",
    "dataset = dataset.map(\n",
    "    lambda batch: processor(images=batch['image'], return_tensors='pt'),\n",
    "    batched=True, batch_size=1024, remove_columns=['image'],\n",
    ")\n",
    "dataset.set_format(type=\"torch\")\n",
    "flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be0b20d0-44e3-4873-868e-9f9741223c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = TrainingArgumentsConfig.get_config(\n",
    "    setup_type=Setups.EVAL_ONLY,\n",
    "    precision_dtype=setup.get_precision_dtype(),\n",
    "    eval_full_precision=setup.get_eval_full_precision(),\n",
    ")\n",
    "default_args['do_eval'] = False\n",
    "default_args['do_predict'] = True\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    per_device_eval_batch_size=setup.get_eval_batch_size(),\n",
    "    **default_args,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=wrapped_model,\n",
    "    args=trainer_args,\n",
    "    compute_metrics=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c07b7d-d150-454a-94b5-b512e4a9e7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/49 22:31 < 12:20, 0.02 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeds = trainer.predict(dataset.select_columns(['pixel_values']))\n",
    "dataset = dataset.remove_columns(['pixel_values']).add_column(name='image_embeds', column=embeds.tolist())\n",
    "dataset.set_format(type='torch', columns=['image_embeds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90bed18-a390-4a96-8fb0-33fb53b2c817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
